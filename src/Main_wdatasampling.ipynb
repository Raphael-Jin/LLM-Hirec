{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\aa\\\\myy\\\\Scripts\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.executable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aa\\AppData\\Local\\Temp\\ipykernel_16824\\2731742536.py:4: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\aa\\AppData\\Local\\Temp\\ipykernel_16824\\2731742536.py:6: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    " \n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True  \n",
    "session = tf.Session(config=config)\n",
    " \n",
    "KTF.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypers import *\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "\n",
    "def trans2tsp(timestr):\n",
    "    return int(time.mktime(datetime.strptime(timestr, '%m/%d/%Y %I:%M:%S %p').timetuple()))\n",
    "\n",
    "def newsample(nnn,ratio):\n",
    "    if ratio >len(nnn):\n",
    "        return random.sample(nnn*(ratio//len(nnn)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(nnn,ratio)\n",
    "\n",
    "def shuffle(pn,labeler,pos):\n",
    "    index=np.arange(pn.shape[0])\n",
    "    pn=pn[index]\n",
    "    labeler=labeler[index]\n",
    "    pos=pos[index]\n",
    "    \n",
    "    for i in range(pn.shape[0]):\n",
    "        index=np.arange(npratio+1)\n",
    "        pn[i,:]=pn[i,index]\n",
    "        labeler[i,:]=labeler[i,index]\n",
    "    return pn,labeler,pos\n",
    "\n",
    "def read_news(path,filenames):\n",
    "    news={}\n",
    "    category=[]\n",
    "    subcategory=[]\n",
    "    news_index={}\n",
    "    index=1\n",
    "    word_dict={}\n",
    "    word_index=1\n",
    "    with open(os.path.join(path,filenames), encoding='utf-8') as f:\n",
    "        lines=f.readlines()\n",
    "    for line in lines:\n",
    "        splited = line.strip('\\n').split('\\t')\n",
    "        doc_id,vert,subvert,title= splited[0:4]\n",
    "        news_index[doc_id]=index\n",
    "        index+=1\n",
    "        category.append(vert)\n",
    "        subcategory.append(subvert)\n",
    "        title = title.lower()\n",
    "        title=word_tokenize(title)\n",
    "        news[doc_id]=[vert,subvert,title]\n",
    "        for word in title:\n",
    "            word = word.lower()\n",
    "            if not(word in word_dict):\n",
    "                word_dict[word]=word_index\n",
    "                word_index+=1\n",
    "    category=list(set(category))\n",
    "    subcategory=list(set(subcategory))\n",
    "    category_dict={}\n",
    "    index=1\n",
    "    for c in category:\n",
    "        category_dict[c]=index\n",
    "        index+=1\n",
    "    subcategory_dict={}\n",
    "    index=1\n",
    "    for c in subcategory:\n",
    "        subcategory_dict[c]=index\n",
    "        index+=1\n",
    "    return news,news_index,category_dict,subcategory_dict,word_dict\n",
    "\n",
    "def get_doc_input(news,news_index,category,subcategory,word_dict):\n",
    "    news_num=len(news)+1\n",
    "    news_title=np.zeros((news_num,MAX_SENTENCE),dtype='int32')\n",
    "    news_vert=np.zeros((news_num,),dtype='int32')\n",
    "    news_subvert=np.zeros((news_num,),dtype='int32')\n",
    "    for key in news:    \n",
    "        vert,subvert,title=news[key]\n",
    "        doc_index=news_index[key]\n",
    "        news_vert[doc_index]=category[vert]\n",
    "        news_subvert[doc_index]=subcategory[subvert]\n",
    "        for word_id in range(min(MAX_SENTENCE,len(title))):\n",
    "            news_title[doc_index,word_id]=word_dict[title[word_id].lower()]\n",
    "    return news_title,news_vert,news_subvert\n",
    "\n",
    "def load_matrix(embedding_path,word_dict):\n",
    "    embedding_matrix = np.zeros((len(word_dict)+1,300))\n",
    "    have_word=[]\n",
    "    with open(os.path.join(embedding_path,'glove.840B.300d.txt'),'rb') as f:\n",
    "        while True:\n",
    "            l=f.readline()\n",
    "            if len(l)==0:\n",
    "                break\n",
    "            l=l.split()\n",
    "            word = l[0].decode()\n",
    "            if word in word_dict:\n",
    "                index = word_dict[word]\n",
    "                tp = [float(x) for x in l[1:]]\n",
    "                embedding_matrix[index]=np.array(tp)\n",
    "                have_word.append(word)\n",
    "    return embedding_matrix,have_word\n",
    "\n",
    "def read_clickhistory_with_click_counts(news_index, data_root_path, filename):\n",
    "    with open(os.path.join(data_root_path, filename)) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    sessions = []\n",
    "    global_click_counts = {}  # Dictionary to store global click counts\n",
    "\n",
    "    for line in lines:\n",
    "        _, uid, eventime, click, imps = line.strip().split('\\t')\n",
    "\n",
    "        if click == '':\n",
    "            clicks = []\n",
    "        else:\n",
    "            clicks = click.split()\n",
    "\n",
    "        true_click = []\n",
    "        for click in clicks:\n",
    "            if click in news_index:\n",
    "                true_click.append(click)\n",
    "                global_click_counts[click] = global_click_counts.get(click, 0) + 1  # Counting clicks\n",
    "\n",
    "        pos = []\n",
    "        neg = []\n",
    "        for imp in imps.split():\n",
    "            docid, label = imp.split('-')\n",
    "            if label == '1':\n",
    "                pos.append(docid)\n",
    "            else:\n",
    "                neg.append(docid)\n",
    "\n",
    "        sessions.append([true_click, pos, neg])\n",
    "\n",
    "    return sessions, global_click_counts\n",
    "\n",
    "\n",
    "def parse_user(news_index,session):\n",
    "    user_num = len(session)\n",
    "    user={'click': np.zeros((user_num,MAX_ALL),dtype='int32'),}\n",
    "    for user_id in range(len(session)):\n",
    "        tclick = []\n",
    "        click, pos, neg =session[user_id]\n",
    "        for i in range(len(click)):\n",
    "            tclick.append(news_index[click[i]])\n",
    "        click = tclick\n",
    "\n",
    "        if len(click) >MAX_ALL:\n",
    "            click = click[-MAX_ALL:]\n",
    "        else:\n",
    "            click=[0]*(MAX_ALL-len(click)) + click\n",
    "            \n",
    "        user['click'][user_id] = np.array(click)\n",
    "    return user\n",
    "\n",
    "def get_train_input(news_index,session,global_click_counts):\n",
    "    sess_pos = []\n",
    "    sess_neg = []\n",
    "    user_id = []\n",
    "    sorted_news_by_clicks = sorted(global_click_counts, key=global_click_counts.get, reverse=True)\n",
    "\n",
    "    for sess_id in range(len(session)):\n",
    "        sess = session[sess_id]\n",
    "        _, poss, negs=sess\n",
    "        for i in range(len(poss)):\n",
    "            pos = poss[i]\n",
    "            neg = newsample(sorted_news_by_clicks, npratio)\n",
    "            sess_pos.append(pos)\n",
    "            sess_neg.append(neg)\n",
    "            user_id.append(sess_id)\n",
    "    sess_all = np.zeros((len(sess_pos),1+npratio),dtype='int32')\n",
    "    label = np.zeros((len(sess_pos),1+npratio))\n",
    "    for sess_id in range(sess_all.shape[0]):\n",
    "        pos = sess_pos[sess_id]\n",
    "        negs = sess_neg[sess_id]\n",
    "        sess_all[sess_id,0] = news_index[pos]\n",
    "        index = 1\n",
    "        for neg in negs:\n",
    "            sess_all[sess_id,index] = news_index[neg]\n",
    "            index+=1\n",
    "        label[sess_id,0]=1\n",
    "    user_id = np.array(user_id, dtype='int32')\n",
    "    \n",
    "    return sess_all, user_id, label\n",
    "\n",
    "def get_test_input(news_index,session):\n",
    "    \n",
    "    Impressions = []\n",
    "    userid = []\n",
    "    for sess_id in range(len(session)):\n",
    "        _, poss, negs = session[sess_id]\n",
    "        imp = {'labels':[],\n",
    "                'docs':[]}\n",
    "        userid.append(sess_id)\n",
    "        for i in range(len(poss)):\n",
    "            docid = news_index[poss[i]]\n",
    "            imp['docs'].append(docid)\n",
    "            imp['labels'].append(1)\n",
    "        for i in range(len(negs)):\n",
    "            docid = news_index[negs[i]]\n",
    "            imp['docs'].append(docid)\n",
    "            imp['labels'].append(0)\n",
    "        Impressions.append(imp)\n",
    "        \n",
    "    userid = np.array(userid,dtype='int32')\n",
    "    \n",
    "    return Impressions, userid,\n",
    "\n",
    "def load_news_entity(news_index,KG_root_path):\n",
    "    with open(os.path.join(KG_root_path,'Release_Small_title.tsv')) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    EntityId2Index = {}\n",
    "    ctt = 1\n",
    "    \n",
    "    news_entity = {}\n",
    "    g = []\n",
    "    for i in range(len(lines)):\n",
    "        d = json.loads(lines[i].strip('\\n'))\n",
    "        docid = d['doc_id']\n",
    "        if not docid in news_index:\n",
    "            continue\n",
    "        news_entity[docid] = []\n",
    "        entities = d['entities']\n",
    "        for j in range(len(entities)):\n",
    "            e = entities[j]['Label']\n",
    "            eid = entities[j]['WikidataId']\n",
    "            if not eid in EntityId2Index:\n",
    "                EntityId2Index[eid] = ctt\n",
    "                ctt += 1\n",
    "            news_entity[docid].append([e,eid,EntityId2Index[eid]])\n",
    "    \n",
    "    meta_news_entity = {}\n",
    "    news_entity2 = {}\n",
    "    \n",
    "    \n",
    "    news_entity_id = {}\n",
    "    for nid in news_entity:\n",
    "        news_entity_id[nid] = []\n",
    "        for e in news_entity[nid]:\n",
    "            news_entity_id[nid].append(e[-2])\n",
    "        news_entity_id[nid] = set(news_entity_id[nid])\n",
    "        \n",
    "    \n",
    "    for docid in news_entity:\n",
    "        meta_news_entity[docid] = news_entity[docid]\n",
    "        news_entity2[docid] = []\n",
    "        for v in news_entity[docid]:\n",
    "            news_entity2[docid].append(v[-1])\n",
    "        news_entity2[docid] = list(set(news_entity2[docid]))[:5]\n",
    "        news_entity2[docid] = news_entity2[docid] + [0]*(5-len(news_entity2[docid]))\n",
    "        news_entity2[docid] = np.array(news_entity2[docid])\n",
    "    \n",
    "    news_entity_np = np.zeros((len(news_entity2)+1,5),dtype='int32')\n",
    "    for nid in news_index:\n",
    "        nix = news_index[nid]\n",
    "        news_entity_np[nix] = news_entity2[nid]\n",
    "        \n",
    "    return news_entity_id,news_entity_np,EntityId2Index\n",
    "\n",
    "def load_entity_embedding(KG_root_path,EntityId2Index):\n",
    "    entity_emb = np.zeros((len(EntityId2Index)+1,100))\n",
    "    import pickle\n",
    "    with open(os.path.join(KG_root_path,'title_entity_emb.pkl'),'rb') as f:\n",
    "        title_entity_emb = pickle.load(f)\n",
    "    for eid in EntityId2Index:\n",
    "        eix = EntityId2Index[eid]\n",
    "        entity_emb[eix] = title_entity_emb[eid]\n",
    "    return entity_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from preprocessing import *\n",
    "from models import *\n",
    "from hypers import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import site\n",
    "# import sys\n",
    "\n",
    "# def set_default_encoding():\n",
    "#     sys.setdefaultencoding('utf-8')\n",
    "\n",
    "# site.setquit(set_default_encoding)\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_path = 'D:\\mind\\out'\n",
    "embedding_path = 'D:\\embedding_path'\n",
    "KG_root_path = 'D:\\Hirec\\HieRec_KGData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news,news_index,category_dict,subcategory_dict,word_dict = read_news(data_root_path,'docs.tsv')\n",
    "# Load data\n",
    "\n",
    "news_title,news_vert,news_subvert=get_doc_input(news,news_index,category_dict,subcategory_dict,word_dict)\n",
    "news_entity,news_entity_np,EntityId2Index = load_news_entity(news_index,KG_root_path)\n",
    "news_info = np.concatenate([news_title,news_entity_np],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions, global_click_counts = read_clickhistory_with_click_counts(news_index, data_root_path, 'train.tsv')\n",
    "\n",
    "# Sort the articles based on click counts in descending order\n",
    "sorted_click_counts = sorted(global_click_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# This gives you a list of tuples where the first element is the news article ID and the second is the click count\n",
    "\n",
    "train_session = read_clickhistory(news_index,data_root_path,'train.tsv')\n",
    "train_user = parse_user(news_index,train_session)\n",
    "# Load data\n",
    "# Use the modified get_train_input function\n",
    "train_sess, train_user_id, train_label = get_train_input(news_index, sessions, global_click_counts)\n",
    "\n",
    "# train_sess, train_user_id, train_label = get_train_input(news_index,train_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_session = read_clickhistory(news_index,data_root_path,'test.tsv')\n",
    "test_user = parse_user(news_index,test_session)\n",
    "test_impressions, test_userids = get_test_input(news_index,test_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_word_embedding_matrix, have_word = load_matrix(embedding_path,word_dict)\n",
    "entity_emb_matrix = load_entity_embedding(KG_root_path,EntityId2Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2nid = {}\n",
    "for nid, nix in news_index.items():\n",
    "    index2nid[nix] = nid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_subvert_mask_table = np.zeros((1,len(category_dict),len(subcategory_dict)))\n",
    "for nid in range(1,len(news_vert)):\n",
    "    v = news_vert[nid]-1\n",
    "    sv = news_subvert[nid]-1\n",
    "    vert_subvert_mask_table[0,v,sv] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "class get_hir_train_generator(Sequence):\n",
    "    def __init__(self,mask_prob,news_scoring,index2nid,news_vert, subvert,news_entity, news_entity_id, clicked_news,user_id, news_id, label, batch_size):\n",
    "        self.news_emb = news_scoring\n",
    "        self.vert = news_vert\n",
    "        self.subvert = subvert\n",
    "        self.entity = news_entity\n",
    "        self.entity_id = news_entity_id\n",
    "        self.index2nid = index2nid\n",
    "        \n",
    "        self.clicked_news = clicked_news\n",
    "\n",
    "        self.user_id = user_id\n",
    "        self.doc_id = news_id\n",
    "        self.label = label\n",
    "        \n",
    "        self.mask_prob = mask_prob\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.ImpNum = self.label.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.ImpNum / float(self.batch_size)))\n",
    "    \n",
    "    def __get_news(self,docids):\n",
    "        news_emb = self.news_emb[docids]\n",
    "        vert = self.vert[docids]\n",
    "        subvert = self.subvert[docids]\n",
    "        entity = self.entity[docids]\n",
    "        return news_emb, vert, subvert, entity\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx*self.batch_size\n",
    "        ed = (idx+1)*self.batch_size\n",
    "        if ed> self.ImpNum:\n",
    "            ed = self.ImpNum\n",
    "            \n",
    "        label = self.label[start:ed]\n",
    "\n",
    "        doc_ids = self.doc_id[start:ed]\n",
    "        title, vert, subvert, entity = self.__get_news(doc_ids)\n",
    "        \n",
    "        user_ids = self.user_id[start:ed]\n",
    "        clicked_ids = self.clicked_news[user_ids]\n",
    "        user_title, user_vert, user_subvert, user_entity = self.__get_news(clicked_ids)\n",
    "        \n",
    "        vert_subvert_mask_input = np.zeros((len(user_subvert),len(category_dict),len(subcategory_dict),))\n",
    "        for bid in range(len(user_subvert)):\n",
    "            for nid in range(len(user_subvert[bid])):\n",
    "                sv = user_subvert[bid][nid]\n",
    "                if sv ==0:\n",
    "                    continue\n",
    "                sv -= 1\n",
    "                vert_subvert_mask_input[bid,:,sv] = 1\n",
    "        vert_subvert_mask_input = vert_subvert_mask_input*vert_subvert_mask_table\n",
    "\n",
    "        \n",
    "        \n",
    "        user_vert = keras.utils.to_categorical(user_vert,len(category_dict)+1)\n",
    "        user_vert = user_vert.transpose((0,2,1))\n",
    "        user_vert = user_vert[:,1:,:]\n",
    "        user_vert_mask = user_vert.sum(axis=-1)\n",
    "        \n",
    "        vert = keras.utils.to_categorical(vert,len(category_dict)+1)\n",
    "        vert = vert[:,:,1:]\n",
    "        \n",
    "        user_subvert = keras.utils.to_categorical(user_subvert,len(subcategory_dict)+1)\n",
    "        user_subvert = user_subvert.transpose((0,2,1))\n",
    "        user_subvert = user_subvert[:,1:,:]\n",
    "        user_subvert_mask = user_subvert.sum(axis=-1)\n",
    "                \n",
    "        subvert = keras.utils.to_categorical(subvert,len(subcategory_dict)+1)\n",
    "        subvert = subvert[:,:,1:]\n",
    "    \n",
    "        user_vert_num = np.array(user_vert.sum(axis=-1),dtype='int32')\n",
    "        user_subvert_num = np.array(user_subvert.sum(axis=-1),dtype='int32')\n",
    "\n",
    "        user_subvert_mask = np.array(user_subvert_mask>0,dtype='float32')\n",
    "        user_vert_mask = np.array(user_vert_mask>0,dtype='float32')\n",
    "        vert_subvert_mask_input = np.array(vert_subvert_mask_input>0,dtype='float32')\n",
    "        \n",
    "        rw_vert = user_vert_num/(user_vert_num.sum(axis=-1).reshape((len(user_vert_num),1))+10**(-8)) #(bz,18)\n",
    "        rw_subvert = user_subvert_num/(user_subvert_num.sum(axis=-1).reshape((len(user_subvert_num),1))+10**(-8)) #(bz,300)\n",
    "        \n",
    "        \n",
    "        \n",
    "        rw_vert = rw_vert.reshape((rw_vert.shape[0],1,rw_vert.shape[1]))\n",
    "        rw_subvert = rw_subvert.reshape((rw_subvert.shape[0],1,rw_subvert.shape[1])) #(bz,1,18)\n",
    "        \n",
    "        rw_vert = (rw_vert*vert).sum(axis=-1)\n",
    "        rw_subvert = (rw_subvert*subvert).sum(axis=-1)\n",
    "        \n",
    "        train_mask = np.random.uniform(0,1,size=(ed-start,1)) > self.mask_prob\n",
    "        train_mask = np.array(train_mask,dtype='float32')\n",
    "        \n",
    "        rw_vert = rw_vert*train_mask\n",
    "        rw_subvert = rw_subvert*train_mask\n",
    "\n",
    "\n",
    "\n",
    "        return ([title,vert,subvert,user_title, user_vert,user_vert_mask,user_subvert,user_subvert_mask,vert_subvert_mask_input,user_vert_num,user_subvert_num,rw_vert,rw_subvert],[label])\n",
    "    \n",
    "    \n",
    "class get_hir_user_generator(Sequence):\n",
    "    def __init__(self,news_emb,news_vert,news_subvert,news_entity, clicked_news,batch_size):\n",
    "        self.news_emb = news_emb\n",
    "        self.vert = news_vert\n",
    "        self.subvert = news_subvert\n",
    "        self.entity = news_entity\n",
    "        \n",
    "        self.clicked_news = clicked_news\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.ImpNum = self.clicked_news.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.ImpNum / float(self.batch_size)))\n",
    "\n",
    "    \n",
    "    def __get_news(self,docids):\n",
    "        news_emb = self.news_emb[docids]\n",
    "        vert = self.vert[docids]\n",
    "        subvert = self.subvert[docids]\n",
    "        entity = self.entity[docids]\n",
    "        return news_emb, vert, subvert, entity\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx*self.batch_size\n",
    "        ed = (idx+1)*self.batch_size\n",
    "        if ed> self.ImpNum:\n",
    "            ed = self.ImpNum\n",
    "            \n",
    "        clicked_ids = self.clicked_news[start:ed]\n",
    "        user_title, user_vert, user_subvert, user_entity = self.__get_news(clicked_ids)\n",
    "        \n",
    "        vert_subvert_mask_input = np.zeros((len(user_subvert),len(category_dict),len(subcategory_dict),))\n",
    "        for bid in range(len(user_subvert)):\n",
    "            for nid in range(len(user_subvert[bid])):\n",
    "                sv = user_subvert[bid][nid]\n",
    "                if sv ==0:\n",
    "                    continue\n",
    "                sv -= 1\n",
    "                vert_subvert_mask_input[bid,:,sv] = 1\n",
    "        vert_subvert_mask_input = vert_subvert_mask_input*vert_subvert_mask_table\n",
    "\n",
    "        \n",
    "        \n",
    "        user_vert = keras.utils.to_categorical(user_vert,len(category_dict)+1)\n",
    "        user_vert = user_vert.transpose((0,2,1))\n",
    "        user_vert = user_vert[:,1:,:]\n",
    "        user_vert_mask = user_vert.sum(axis=-1)\n",
    "        \n",
    "        \n",
    "        user_subvert = keras.utils.to_categorical(user_subvert,len(subcategory_dict)+1)\n",
    "        user_subvert = user_subvert.transpose((0,2,1))\n",
    "        user_subvert = user_subvert[:,1:,:]\n",
    "        user_subvert_mask = user_subvert.sum(axis=-1)\n",
    "        \n",
    "        user_vert_num = np.array(user_vert.sum(axis=-1),dtype='int32')\n",
    "        user_subvert_num = np.array(user_subvert.sum(axis=-1),dtype='int32')\n",
    "        \n",
    "        user_subvert_mask = np.array(user_subvert_mask>0,dtype='float32')\n",
    "        user_vert_mask = np.array(user_vert_mask>0,dtype='float32')\n",
    "        vert_subvert_mask_input = np.array(vert_subvert_mask_input>0,dtype='float32')\n",
    "\n",
    "        return [user_title, user_vert,user_vert_mask,user_subvert,user_subvert_mask,vert_subvert_mask_input,user_vert_num,user_subvert_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_combine2(test_impressions,users,user_subvert_rep,user_vert_rep,user_global_rep,w1,w2,w3):\n",
    "    AUC = []\n",
    "    MRR = []\n",
    "    nDCG5 = []\n",
    "    nDCG10 =[]\n",
    "    for i in range(len(test_impressions)):\n",
    "        labels = test_impressions[i]['labels']\n",
    "        nids = test_impressions[i]['docs']\n",
    "        verts = news_vert[nids]\n",
    "        verts = verts-1\n",
    "        subverts = news_subvert[nids]\n",
    "        subverts = subverts-1\n",
    "\n",
    "        user_gv = user_global_rep[i]\n",
    "        user_vv = user_vert_rep[i]\n",
    "        user_svv = user_subvert_rep[i]\n",
    "\n",
    "        click = users[i]\n",
    "        \n",
    "        nv = news_scoring[nids]\n",
    "        score1 = np.dot(nv,user_gv)\n",
    "        user_vv = user_vv[verts]\n",
    "        score2 = (nv*user_vv).sum(axis=-1)\n",
    "        \n",
    "        mask2 = []\n",
    "        for v in verts:\n",
    "            t = news_vert[click]==(v+1)\n",
    "            mask2.append(t.sum())\n",
    "        mask2 = np.array(mask2)\n",
    "        mask2 = mask2/((click>0).sum()+10**(-6))\n",
    "        \n",
    "        \n",
    "        user_svv = user_svv[subverts]\n",
    "        score3 = (nv*user_svv).sum(axis=-1)\n",
    "\n",
    "        mask3 = []\n",
    "        for svi in range(len(subverts)):\n",
    "            sv = subverts[svi]\n",
    "            t = (news_subvert[click]==(sv+1))\n",
    "            mask3.append(t.sum())\n",
    "        mask3 = np.array(mask3)\n",
    "        mask3 = mask3/((click>0).sum()+10**(-6))\n",
    "        \n",
    "\n",
    "            \n",
    "        score1 = np.array(score1)\n",
    "        score2 = np.array(score2)\n",
    "        score3 = np.array(score3)\n",
    "\n",
    "        score = score1*w1+mask2*score2*w2+mask3*score3*w3\n",
    "        \n",
    "\n",
    "        auc = roc_auc_score(labels,score)\n",
    "        mrr = mrr_score(labels,score)\n",
    "        ndcg5 = ndcg_score(labels,score,k=5)\n",
    "        ndcg10 = ndcg_score(labels,score,k=10)\n",
    "    \n",
    "        AUC.append(auc)\n",
    "        MRR.append(mrr)\n",
    "        nDCG5.append(ndcg5)\n",
    "        nDCG10.append(ndcg10)\n",
    "\n",
    "    return AUC, MRR, nDCG5, nDCG10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aa\\myy\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\aa\\myy\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\aa\\myy\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\aa\\myy\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\aa\\myy\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\aa\\myy\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\aa\\myy\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\aa\\myy\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\aa\\myy\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\aa\\myy\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\aa\\myy\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\aa\\myy\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\aa\\myy\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\aa\\myy\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/4\n"
     ]
    }
   ],
   "source": [
    "model,news_encoder,user_encoder,rews = create_model(category_dict,subcategory_dict,title_word_embedding_matrix,entity_emb_matrix)\n",
    "Res.append({'AUC':[],'MRR':[],'nDCG5':[],'nDCG10':[]})\n",
    "train_generator = get_hir_train_generator(0.9999,news_info,index2nid,news_vert,news_subvert,news_entity_np,news_entity,train_user['click'],train_user_id,train_sess,train_label,16)\n",
    "model.fit_generator(train_generator,epochs=4,verbose=3)\n",
    "\n",
    "for i in range(1):\n",
    "    model.fit_generator(train_generator,epochs=1,verbose=2)\n",
    "    news_scoring = news_encoder.predict(news_info,verbose=True)\n",
    "    test_user_generator = get_hir_user_generator(news_scoring,news_vert,news_subvert,news_entity_np,test_user['click'],32)\n",
    "    \n",
    "    AUC = []\n",
    "    MRR = []\n",
    "    nDCG5 = []\n",
    "    nDCG10 = []\n",
    "\n",
    "    for i in range(int(np.ceil(len(test_user['click'])/1000))):\n",
    "        start = i*1000\n",
    "        ed = (i+1)*1000\n",
    "        ed = min(ed,len(test_user['click']))\n",
    "        test_user_generator = get_hir_user_generator(news_scoring,news_vert,news_subvert,news_entity_np,test_user['click'][start:ed],32)\n",
    "        user_subvert_rep,user_vert_rep,user_global_rep = user_encoder.predict_generator(test_user_generator,verbose=False)\n",
    "        a,m,n5,n10 = evaluate_combine2(test_impressions[start:ed],test_user['click'][start:ed],user_subvert_rep,user_vert_rep,user_global_rep,0.15,0.15,0.7)\n",
    "        AUC += a\n",
    "        MRR += m\n",
    "        nDCG5 += n5\n",
    "        nDCG10 += n10\n",
    "\n",
    "        print(np.array(AUC).mean(),np.array(MRR).mean(),np.array(nDCG5).mean(),np.array(nDCG10).mean())\n",
    "\n",
    "\n",
    "\n",
    "    #     break\n",
    "    AUC = np.array(AUC)\n",
    "    MRR = np.array(MRR)\n",
    "    nDCG5 = np.array(nDCG5)\n",
    "    nDCG10 = np.array(nDCG10)\n",
    "\n",
    "    AUC = AUC.mean()\n",
    "    MRR = MRR.mean()\n",
    "    nDCG5 = nDCG5.mean()\n",
    "    nDCG10 = nDCG10.mean()\n",
    "    \n",
    "    Res[-1]['AUC'].append(AUC)\n",
    "    Res[-1]['MRR'].append(MRR)\n",
    "    Res[-1]['nDCG5'].append(nDCG5)\n",
    "    Res[-1]['nDCG10'].append(nDCG10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myy",
   "language": "python",
   "name": "myy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
